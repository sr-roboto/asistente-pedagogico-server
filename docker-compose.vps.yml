version: '3.8'

services:
  # RAG API Server
  server:
    build:
      context: ./server
      dockerfile: Dockerfile
    restart: unless-stopped
    ports:
      - "8000:8000"
    env_file:
      - ./server/.env
    environment:
      - APP_ENV=production
      - HOST=0.0.0.0
      - OLLAMA_BASE_URL=http://ollama:11434
      - AI_PROVIDER=gemini
      - PORT=8000
      - DATABASE_URL=postgresql://postgres:postgres@postgres:5432/asistente_db
    volumes:
      # Persist FAISS indexes (Vector DB)
      # Matching the paths defined in rag_service.py specific to fastembed
      - ./server/faiss_index_ollama_local:/app/faiss_index_ollama_local
      - ./server/faiss_index_gemini_local:/app/faiss_index_gemini_local
      - ./server/faiss_index_deepseek_local:/app/faiss_index_deepseek_local
      # Persist raw data (PDFs)
      - ./server/data:/app/data

    depends_on:
      ollama:
        condition: service_started
      postgres:
        condition: service_healthy
    networks:
      - tomi-network

  # Ollama Service (The LLM Host)
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    container_name: ollama
    volumes:
      - ollama_storage:/root/.ollama
    networks:
      - tomi-network
    # Expose port mostly for debugging, internal communication happens via network
    expose:
      - "11434"

  postgres:
    image: postgres:15-alpine
    restart: unless-stopped
    container_name: postgres
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=asistente_db
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres" ]
      interval: 5s
      timeout: 5s
      retries: 5
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - tomi-network

volumes:
  ollama_storage:
  postgres_data:


networks:
  tomi-network:
    driver: bridge
